{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9839bf-4275-4bfa-a106-29c1b68aca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "def fetch_data(source, source_type, file_type):\n",
    "    if source_type == 'url':\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(source)\n",
    "        elif file_type == 'json':\n",
    "            df = pd.read_json(source)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Please use 'csv' or 'json'.\")\n",
    "    elif source_type == 'local':\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(source)\n",
    "        elif file_type == 'json':\n",
    "            df = pd.read_json(source)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Please use 'csv' or 'json'.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported source type. Please use 'url' or 'local'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca40764-1b21-4546-b67c-f836d964457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_format(df, output_format='csv', output_file=None):\n",
    "    # Convert the DataFrame to the desired format\n",
    "    if output_format == 'json':\n",
    "        result = df.to_json(orient='records', lines=True)\n",
    "    elif output_format == 'csv':\n",
    "        result = df.to_csv(index=False)\n",
    "    elif output_format == 'sql':\n",
    "        if output_file:\n",
    "            conn = sqlite3.connect(output_file)\n",
    "            df.to_sql('data_table', conn, if_exists='replace', index=False)\n",
    "            conn.close()\n",
    "            return\n",
    "        else:\n",
    "            raise ValueError(\"SQL format requires output_file path for the database.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported output format\")\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bd0dc2-2128-4a70-aaba-35e8e68da60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_columns(df, columns_to_add=None, columns_to_remove=None):\n",
    "    # Add or remove columns from the DataFrame\n",
    "    if columns_to_remove:\n",
    "        df = df.drop(columns=columns_to_remove)\n",
    "    if columns_to_add:\n",
    "        for col, val in columns_to_add.items():\n",
    "            if col == 'High_low_diff':\n",
    "                # Calculate difference between 'High' and 'Low' columns\n",
    "                df[col] = df['High'] - df['Low']\n",
    "            else:\n",
    "                df[col] = val\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b8cb30-3885-43bb-ba3b-2c79fa0fb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_data(df, stage='Pre-Processing'):\n",
    "    # Generate a summary of the DataFrame\n",
    "    print(f\"Summary of {stage} Data:\")\n",
    "    print(f\"Number of records: {df.shape[0]}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca98d25-ebe2-4364-9696-28bd9a8b4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline():\n",
    "    try:\n",
    "        # Input section for the user to specify details about the source\n",
    "        source = input(\"Enter the data source (URL or local file path): \")\n",
    "        source_type = input(\"Enter the source type (url/local): \").lower()\n",
    "        file_type = input(\"Enter the file type (csv/json): \").lower()\n",
    "        output_format = input(\"Enter the output format (csv/json/sql): \").lower()\n",
    "        output_file = input(\"Enter the output file name with extension (e.g., output.csv or output.db): \")\n",
    "        \n",
    "        # Asking user whether they want to add or remove columns\n",
    "        modify_choice = input(\"Do you want to modify columns? (yes/no): \").lower()\n",
    "        \n",
    "        columns_to_add = None\n",
    "        columns_to_remove = None\n",
    "        \n",
    "        if modify_choice == 'yes':\n",
    "            add_columns = input(\"Enter columns to add (format: {'new_column_name': 'default_value'}, or leave empty if none): \")\n",
    "            remove_columns = input(\"Enter columns to remove (comma-separated list, or leave empty if none): \")\n",
    "            \n",
    "            if add_columns:\n",
    "                columns_to_add = eval(add_columns)  # Convert string input to a dictionary\n",
    "            if remove_columns:\n",
    "                columns_to_remove = remove_columns.split(',')\n",
    "        \n",
    "        # Call the ETL functions with user inputs\n",
    "        df = fetch_data(source, source_type, file_type)\n",
    "        summarize_data(df, stage='Pre-Processing')\n",
    "    \n",
    "        # Modify columns based on user inputs\n",
    "        df = modify_columns(df, columns_to_add, columns_to_remove)\n",
    "        \n",
    "        # Store the converted file\n",
    "        convert_data_format(df, output_format, output_file)\n",
    "        summarize_data(df, stage='Post-Processing')\n",
    "        \n",
    "        print(\"ETL Process Completed Successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843bed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the data source (URL or local file path): sap500.csv\n",
      "Enter the source type (url/local): local\n",
      "Enter the file type (csv/json): csv\n",
      "Enter the output format (csv/json/sql): sql\n",
      "Enter the output file name with extension (e.g., output.csv or output.db): output.db\n",
      "Do you want to modify columns? (yes/no): yes\n",
      "Enter columns to add (format: {'new_column_name': 'default_value'}, or leave empty if none): {'High_low_diff': 'Calculated'}\n",
      "Enter columns to remove (comma-separated list, or leave empty if none): Volume\n",
      "Summary of Pre-Processing Data:\n",
      "Number of records: 24315\n",
      "Number of columns: 6\n",
      "         Date       Open       High        Low      Close  Volume\n",
      "0  1927-12-30  17.660000  17.660000  17.660000  17.660000       0\n",
      "1  1928-01-03  17.760000  17.760000  17.760000  17.760000       0\n",
      "2  1928-01-04  17.719999  17.719999  17.719999  17.719999       0\n",
      "3  1928-01-05  17.549999  17.549999  17.549999  17.549999       0\n",
      "4  1928-01-06  17.660000  17.660000  17.660000  17.660000       0\n",
      "Summary of Post-Processing Data:\n",
      "Number of records: 24315\n",
      "Number of columns: 6\n",
      "         Date       Open       High        Low      Close  High_low_diff\n",
      "0  1927-12-30  17.660000  17.660000  17.660000  17.660000            0.0\n",
      "1  1928-01-03  17.760000  17.760000  17.760000  17.760000            0.0\n",
      "2  1928-01-04  17.719999  17.719999  17.719999  17.719999            0.0\n",
      "3  1928-01-05  17.549999  17.549999  17.549999  17.549999            0.0\n",
      "4  1928-01-06  17.660000  17.660000  17.660000  17.660000            0.0\n",
      "ETL Process Completed Successfully.\n"
     ]
    }
   ],
   "source": [
    "source = 'sap500.csv'  # Change to your CSV or JSON source\n",
    "source_type = 'local'  # or 'local'\n",
    "file_type = 'csv'  # 'csv'or 'json'\n",
    "output_format = 'sql'  # 'csv' or 'json', 'sql'\n",
    "output_file = 'output.db'  # 'output.csv' or 'output.db' for SQL\n",
    "\n",
    "columns_to_add = {'High_low_diff': 'Calculated'}\n",
    "columns_to_remove = ['Volume']\n",
    "\n",
    "etl_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a0ea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the data source (URL or local file path): https://financialmodelingprep.com/api/v3/stock/list?apikey=2lewNDBSw4SM8weBDYlB6UEN8AcqUUKM\n",
      "Enter the source type (url/local): url\n",
      "Enter the file type (csv/json): json\n",
      "Enter the output format (csv/json/sql): sql\n",
      "Enter the output file name with extension (e.g., output.csv or output.db): output.db\n",
      "Do you want to modify columns? (yes/no): yes\n",
      "Enter columns to add (format: {'new_column_name': 'default_value'}, or leave empty if none): \n",
      "Enter columns to remove (comma-separated list, or leave empty if none): exchangeShortName\n",
      "Summary of Pre-Processing Data:\n",
      "Number of records: 84178\n",
      "Number of columns: 6\n",
      "      symbol                                               name   price  \\\n",
      "0  PMGOLD.AX                                    Perth Mint Gold   17.94   \n",
      "1     FEMACX  The First Trust Combined Series 447: Investmen...  943.94   \n",
      "2      XSIAX                            Voya Credit Income Fund    9.70   \n",
      "3      YACKX                          AMG Yacktman Fund Class I   25.53   \n",
      "4      ZSCCX                          Zacks Small-Cap Core Fund   38.01   \n",
      "\n",
      "                         exchange exchangeShortName   type  \n",
      "0  Australian Securities Exchange               ASX    etf  \n",
      "1                          NASDAQ            NASDAQ  trust  \n",
      "2                          Nasdaq            NASDAQ  trust  \n",
      "3                          NASDAQ            NASDAQ  trust  \n",
      "4                          NASDAQ            NASDAQ  trust  \n",
      "Summary of Post-Processing Data:\n",
      "Number of records: 84178\n",
      "Number of columns: 5\n",
      "      symbol                                               name   price  \\\n",
      "0  PMGOLD.AX                                    Perth Mint Gold   17.94   \n",
      "1     FEMACX  The First Trust Combined Series 447: Investmen...  943.94   \n",
      "2      XSIAX                            Voya Credit Income Fund    9.70   \n",
      "3      YACKX                          AMG Yacktman Fund Class I   25.53   \n",
      "4      ZSCCX                          Zacks Small-Cap Core Fund   38.01   \n",
      "\n",
      "                         exchange   type  \n",
      "0  Australian Securities Exchange    etf  \n",
      "1                          NASDAQ  trust  \n",
      "2                          Nasdaq  trust  \n",
      "3                          NASDAQ  trust  \n",
      "4                          NASDAQ  trust  \n",
      "ETL Process Completed Successfully.\n"
     ]
    }
   ],
   "source": [
    "source = 'https://financialmodelingprep.com/api/v3/stock/list?apikey=2lewNDBSw4SM8weBDYlB6UEN8AcqUUKM'  # Change to your CSV or JSON source\n",
    "source_type = 'url'  # or 'local'\n",
    "file_type = 'json'  # 'csv'or 'json'\n",
    "output_format = 'sql'  # 'csv' or 'json', 'sql'\n",
    "output_file = 'output.db'  # 'output.csv' or 'output.db' for SQL\n",
    "columns_to_add = None\n",
    "columns_to_remove = ['exchangeShortName']\n",
    "\n",
    "etl_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
